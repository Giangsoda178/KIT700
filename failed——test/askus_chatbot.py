import requests
from bs4 import BeautifulSoup
import time
import json
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

# -------- Step 1: Crawl FAQ --------
def crawl_askus_faq(keyword="enrol", max_pages=3, sleep_sec=1):
    base_url = "https://askus.utas.edu.au"
    search_url = f"{base_url}/app/answers/list/kw/{keyword}/page/{{page}}"
    headers = {"User-Agent": "Mozilla/5.0"}
    faq_data = []

    for page in range(1, max_pages + 1):
        print(f"ğŸ“¥ Crawling page {page}...")
        try:
            res = requests.get(search_url.format(page=page), headers=headers, timeout=10)
            soup = BeautifulSoup(res.text, 'html.parser')
            links = soup.select("div.answer_text a[href*='/app/answers/detail/']")

            if not links:
                print("âš ï¸ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•é“¾æ¥ï¼Œå¯èƒ½è¢«é‡å®šå‘æˆ–é¡µé¢ä¸ºç©ºã€‚")

            for link in links:
                title = link.text.strip()
                href = base_url + link.get("href")
                print(f"ğŸ”— æŠ“å–é—®é¢˜ï¼š{title}")
                detail = requests.get(href, headers=headers, timeout=10)
                detail_soup = BeautifulSoup(detail.text, 'html.parser')
                answer_div = detail_soup.select_one(".answer_text")
                answer = answer_div.get_text(strip=True) if answer_div else ""
                faq_data.append({
                    "question": title,
                    "answer": answer,
                    "url": href
                })
                time.sleep(sleep_sec)
        except Exception as e:
            print(f"âš ï¸ é¡µé¢ {page} æŠ“å–å¤±è´¥ï¼š{e}")
    
    with open("faq_data.json", "w") as f:
        json.dump(faq_data, f, indent=2, ensure_ascii=False)
    print("âœ… FAQ æ•°æ®å·²ä¿å­˜è‡³ faq_data.json")
    return faq_data

# -------- Step 2: TF-IDF æ£€ç´¢ --------
def load_faq_data():
    with open("faq_data.json") as f:
        return json.load(f)

def search_faq(user_query, data, top_n=1):
    questions = [item["question"] for item in data if item["question"].strip()]
    if not questions:
        raise ValueError("âŒ FAQ æ•°æ®ä¸­æ²¡æœ‰æœ‰æ•ˆé—®é¢˜æ–‡æœ¬ã€‚")

    vectorizer = TfidfVectorizer().fit(questions + [user_query])
    q_vecs = vectorizer.transform(questions)
    user_vec = vectorizer.transform([user_query])
    sims = cosine_similarity(user_vec, q_vecs).flatten()
    top_indices = sims.argsort()[-top_n:][::-1]
    return [data[i] for i in top_indices]

# -------- Step 3: Console Chatbot --------
def main():
    print("ğŸ“š UTAS AskUs FAQ Chatbot (Console)")
    choice = input("æ˜¯å¦éœ€è¦é‡æ–°æŠ“å– FAQ æ•°æ®ï¼Ÿ(y/n): ").strip().lower()
    if choice == 'y':
        data = crawl_askus_faq(keyword="enrol", max_pages=3)
    else:
        data = load_faq_data()

    if not data:
        print("âŒ FAQ æ•°æ®ä¸ºç©ºï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–é‡æ–°çˆ¬å–ã€‚")
        return

    while True:
        query = input("\nä½ æƒ³é—®ä»€ä¹ˆï¼Ÿ(è¾“å…¥ 'exit' é€€å‡º)\n> ")
        if query.lower() == "exit":
            break
        try:
            results = search_faq(query, data)
            if results:
                res = results[0]
                print("\nâœ… æœ€ç›¸å…³çš„é—®é¢˜ï¼š", res["question"])
                print("ğŸ“– å›ç­”é¢„è§ˆï¼š", res["answer"][:300] + "..." if len(res["answer"]) > 300 else res["answer"])
                print("ğŸ”— é“¾æ¥ï¼š", res["url"])
            else:
                print("âŒ æ²¡æœ‰æ‰¾åˆ°ç›¸å…³é—®é¢˜ã€‚")
        except Exception as e:
            print("âš ï¸ é”™è¯¯ï¼š", e)

if __name__ == "__main__":
    main()
